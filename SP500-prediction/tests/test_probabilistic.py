import unittest
import numpy as np
import pandas as pd
import sys
import os

# Ajouter le chemin src au PYTHONPATH
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from probabilistic import ProbabilisticForecaster
from data_prep import DataPreprocessor

class TestProbabilisticModels(unittest.TestCase):
    
    def setUp(self):
        """Setup pour les tests probabilistes"""
        self.prob_forecaster = ProbabilisticForecaster()
        
        # G√©n√©ration de donn√©es financi√®res simul√©es pour les tests
        np.random.seed(42)
        dates = pd.date_range('2020-01-01', periods=1000, freq='D')
        
        # Simulation de prix avec tendance et saisonnalit√©
        trend = np.linspace(100, 500, 1000)
        noise = np.random.normal(0, 10, 1000)
        seasonal = 50 * np.sin(2 * np.pi * np.arange(1000) / 365)
        
        self.test_data = pd.DataFrame({
            'Close': trend + seasonal + noise,
            'Volume': np.random.lognormal(10, 1, 1000),
            'interest_rate': np.random.normal(2.5, 0.5, 1000),
            'inflation': np.random.normal(2.0, 0.3, 1000),
            'vix': np.random.normal(15, 5, 1000)
        }, index=dates)
        
        # Ajout des returns et volatilit√© pour les tests
        self.test_data['returns'] = self.test_data['Close'].pct_change()
        self.test_data['volatility_20'] = self.test_data['returns'].rolling(20).std()
        self.test_data['rsi_14'] = self._calculate_test_rsi(self.test_data['Close'])
        
        # Nettoyage des NaN
        self.test_data = self.test_data.dropna()
        
        # Donn√©es d'entra√Ænement/test
        self.X_train = np.random.randn(800, 5)
        self.X_test = np.random.randn(200, 5)
        self.y_train = np.random.randn(800)
        
    def _calculate_test_rsi(self, prices, window=14):
        """Calcul simplifi√© du RSI pour les tests"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
        rs = gain / loss
        return 100 - (100 / (1 + rs))
    
    def test_gmm_market_regimes(self):
        """Test de la d√©tection des r√©gimes de march√© avec GMM"""
        print("Testing GMM Market Regimes...")
        
        # Test avec diff√©rents nombres de composants
        for n_components in [2, 3, 4]:
            with self.subTest(n_components=n_components):
                gmm_model = self.prob_forecaster.fit_market_regimes(
                    self.test_data, n_components=n_components
                )
                
                # V√©rifications de base
                self.assertIsNotNone(gmm_model)
                self.assertEqual(gmm_model.n_components, n_components)
                self.assertTrue(hasattr(gmm_model, 'means_'))
                self.assertTrue(hasattr(gmm_model, 'covariances_'))
                
                # V√©rifier que le mod√®le est bien entra√Æn√©
                self.assertTrue(gmm_model.converged_)
                
                print(f"‚úì GMM with {n_components} components trained successfully")
    
    def test_bayesian_inference(self):
        """Test de l'inf√©rence bay√©sienne"""
        print("Testing Bayesian Inference...")
        
        # Test de la r√©gression bay√©sienne
        predictions = self.prob_forecaster.bayesian_inference(
            self.X_train, self.X_test, self.y_train
        )
        
        # V√©rifications
        self.assertIsNotNone(predictions)
        self.assertEqual(len(predictions), len(self.X_test))
        self.assertIsInstance(predictions, np.ndarray)
        
        # V√©rifier que le mod√®le est sauvegard√©
        self.assertIn('bayesian', self.prob_forecaster.models)
        bayesian_model = self.prob_forecaster.models['bayesian']
        self.assertIsNotNone(bayesian_model)
        
        # V√©rifier que le mod√®le a bien appris
        self.assertTrue(hasattr(bayesian_model, 'coef_'))
        
        print("‚úì Bayesian inference completed successfully")
    
    def test_gaussian_process_regression(self):
        """Test de la r√©gression par processus gaussiens"""
        print("Testing Gaussian Process Regression...")
        
        # Test GPR
        y_pred, y_std = self.prob_forecaster.gaussian_process_regression(
            self.X_train, self.X_test, self.y_train
        )
        
        # V√©rifications de base
        self.assertIsNotNone(y_pred)
        self.assertIsNotNone(y_std)
        self.assertEqual(len(y_pred), len(self.X_test))
        self.assertEqual(len(y_std), len(self.X_test))
        
        # V√©rifier que les incertitudes sont positives
        self.assertTrue(np.all(y_std >= 0))
        
        # V√©rifier la coh√©rence des pr√©dictions
        self.assertFalse(np.any(np.isnan(y_pred)))
        self.assertFalse(np.any(np.isnan(y_std)))
        
        print("‚úì Gaussian Process Regression completed successfully")
    
    def test_monte_carlo_simulation(self):
        """Test des simulations Monte Carlo"""
        print("Testing Monte Carlo Simulations...")
        
        # G√©n√©rer des returns simul√©s
        returns = self.test_data['returns'].dropna()
        
        # Tests avec diff√©rents param√®tres
        test_cases = [
            (100, 10),   # 100 simulations, 10 jours
            (500, 30),   # 500 simulations, 30 jours
            (1000, 50)   # 1000 simulations, 50 jours
        ]
        
        for n_simulations, days in test_cases:
            with self.subTest(n_simulations=n_simulations, days=days):
                simulations = self.prob_forecaster.monte_carlo_simulation(
                    returns, n_simulations=n_simulations, days=days
                )
                
                # V√©rifications
                self.assertIsNotNone(simulations)
                self.assertEqual(simulations.shape, (n_simulations, days))
                self.assertFalse(np.any(np.isnan(simulations)))
                
                # V√©rifier que les simulations ont une variance raisonnable
                final_prices = simulations[:, -1]
                price_std = np.std(final_prices)
                self.assertGreater(price_std, 0)  # Doit avoir une certaine variabilit√©
                
                print(f"‚úì Monte Carlo simulation ({n_simulations}, {days}) completed successfully")
    
    def test_value_at_risk(self):
        """Test du calcul de la Value at Risk"""
        print("Testing Value at Risk Calculation...")
        
        returns = self.test_data['returns'].dropna()
        
        # Test avec diff√©rents niveaux de confiance
        confidence_levels = [0.01, 0.05, 0.10]
        
        for confidence in confidence_levels:
            with self.subTest(confidence=confidence):
                var = self.prob_forecaster.calculate_value_at_risk(
                    returns, confidence_level=confidence
                )
                
                # V√©rifications
                self.assertIsNotNone(var)
                self.assertIsInstance(var, (float, np.floating))
                
                # La VaR doit √™tre n√©gative (risque de perte)
                self.assertLess(var, 0)
                
                # V√©rifier que la VaR est dans une plage raisonnable
                returns_min = returns.min()
                self.assertGreaterEqual(var, returns_min)
                
                print(f"‚úì VaR at {confidence*100}% confidence level: {var:.4f}")
    
    def test_probabilistic_predictions_consistency(self):
        """Test de la coh√©rence entre diff√©rents mod√®les probabilistes"""
        print("Testing Probabilistic Models Consistency...")
        
        # Obtenir les pr√©dictions de diff√©rents mod√®les
        bayesian_pred = self.prob_forecaster.bayesian_inference(
            self.X_train, self.X_test, self.y_train
        )
        
        gpr_pred, gpr_std = self.prob_forecaster.gaussian_process_regression(
            self.X_train, self.X_test, self.y_train
        )
        
        # V√©rifier que les pr√©dictions ont la m√™me forme
        self.assertEqual(len(bayesian_pred), len(gpr_pred))
        
        # V√©rifier que les pr√©dictions ne sont pas identiques (diff√©rents mod√®les)
        correlation = np.corrcoef(bayesian_pred, gpr_pred)[0, 1]
        self.assertNotEqual(correlation, 1.0)  # Ne doivent pas √™tre parfaitement corr√©l√©s
        
        # V√©rifier que les √©carts-types de GPR sont raisonnables
        self.assertTrue(np.all(gpr_std >= 0))
        avg_std = np.mean(gpr_std)
        self.assertGreater(avg_std, 0)  # Doit avoir une certaine incertitude
        
        print("‚úì Probabilistic models show consistent but diverse predictions")
    
    def test_edge_cases(self):
        """Test des cas limites pour les mod√®les probabilistes"""
        print("Testing Edge Cases...")
        
        # Test avec tr√®s peu de donn√©es
        small_X = np.random.randn(5, 3)
        small_y = np.random.randn(5)
        
        # Bayesian inference devrait fonctionner m√™me avec peu de donn√©es
        small_pred = self.prob_forecaster.bayesian_inference(
            small_X, small_X, small_y
        )
        self.assertEqual(len(small_pred), len(small_X))
        
        # Test avec donn√©es constantes
        constant_returns = pd.Series([0.01] * 100)  # Returns constants
        var_constant = self.prob_forecaster.calculate_value_at_risk(constant_returns)
        self.assertEqual(var_constant, 0.01)  # VaR devrait √™tre √©gale au return constant
        
        # Test Monte Carlo avec tr√®s peu de simulations
        few_simulations = self.prob_forecaster.monte_carlo_simulation(
            self.test_data['returns'].dropna(), n_simulations=10, days=5
        )
        self.assertEqual(few_simulations.shape, (10, 5))
        
        print("‚úì Edge cases handled successfully")
    
    def test_model_persistence(self):
        """Test de la persistance des mod√®les entra√Æn√©s"""
        print("Testing Model Persistence...")
        
        # Entra√Æner plusieurs mod√®les
        gmm_model = self.prob_forecaster.fit_market_regimes(self.test_data)
        bayesian_pred = self.prob_forecaster.bayesian_inference(
            self.X_train, self.X_test, self.y_train
        )
        
        # V√©rifier que les mod√®les sont bien sauvegard√©s
        self.assertIn('gmm', self.prob_forecaster.models)
        self.assertIn('bayesian', self.prob_forecaster.models)
        
        gmm_saved = self.prob_forecaster.models['gmm']
        bayesian_saved = self.prob_forecaster.models['bayesian']
        
        self.assertIsNotNone(gmm_saved)
        self.assertIsNotNone(bayesian_saved)
        
        # V√©rifier que les mod√®les sauvegard√©s peuvent faire des pr√©dictions
        if hasattr(gmm_saved, 'predict'):
            gmm_predictions = gmm_saved.predict(self.test_data[['returns', 'volatility_20', 'vix', 'rsi_14']].dropna())
            self.assertIsNotNone(gmm_predictions)
        
        bayesian_new_pred = bayesian_saved.predict(self.X_test)
        self.assertEqual(len(bayesian_new_pred), len(self.X_test))
        
        print("‚úì Model persistence verified successfully")

    def test_probabilistic_metrics(self):
        """Test des m√©triques probabilistes sp√©cifiques"""
        print("Testing Probabilistic Metrics...")
        
        # Test de la calibration des incertitudes
        _, gpr_std = self.prob_forecaster.gaussian_process_regression(
            self.X_train, self.X_test, self.y_train
        )
        
        # V√©rifier que l'incertitude est corr√©l√©e avec l'erreur de pr√©diction
        gpr_pred, _ = self.prob_forecaster.gaussian_process_regression(
            self.X_train, self.X_test, self.y_train
        )
        
        # Pour un test significatif, nous aurions besoin de vraies valeurs cibles
        # Ici, nous v√©rifions simplement que l'incertitude est calcul√©e
        uncertainty_mean = np.mean(gpr_std)
        self.assertGreater(uncertainty_mean, 0)
        
        # Test de la distribution des pr√©dictions Monte Carlo
        returns = self.test_data['returns'].dropna()
        simulations = self.prob_forecaster.monte_carlo_simulation(
            returns, n_simulations=1000, days=10
        )
        
        # V√©rifier que la distribution a les propri√©t√©s attendues
        final_returns = simulations[:, -1] - 1  # Convertir en returns
        mean_return = np.mean(final_returns)
        std_return = np.std(final_returns)
        
        # La moyenne devrait √™tre proche du return historique moyen
        historical_mean = returns.mean()
        self.assertAlmostEqual(mean_return, historical_mean, delta=0.1)
        
        print("‚úì Probabilistic metrics calculated correctly")

def run_probabilistic_tests():
    """Fonction pour ex√©cuter tous les tests probabilistes"""
    print("=" * 60)
    print("EX√âCUTION DES TESTS PROBABILISTES")
    print("=" * 60)
    
    # Cr√©er une suite de tests
    loader = unittest.TestLoader()
    suite = loader.loadTestsFromTestCase(TestProbabilisticModels)
    
    # Ex√©cuter les tests avec une sortie verbose
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # Afficher un r√©sum√©
    print("\n" + "=" * 60)
    print("R√âSUM√â DES TESTS PROBABILISTES")
    print("=" * 60)
    print(f"Tests ex√©cut√©s: {result.testsRun}")
    print(f"√âchecs: {len(result.failures)}")
    print(f"Erreurs: {len(result.errors)}")
    
    if result.wasSuccessful():
        print("üéâ TOUS LES TESTS PROBABILISTES ONT R√âUSSI !")
    else:
        print("‚ùå CERTAINS TESTS ONT √âCHOU√â")
        for test, traceback in result.failures + result.errors:
            print(f"\n√âchec dans: {test}")
            print(f"Traceback: {traceback}")
    
    return result.wasSuccessful()

if __name__ == '__main__':
    # Ex√©cuter tous les tests
    success = run_probabilistic_tests()
    
    # Retourner un code de sortie appropri√©
    sys.exit(0 if success else 1)